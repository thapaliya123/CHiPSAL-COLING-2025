{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "%pip install -q torch\n",
    "%pip install -q datasets\n",
    "%pip install -q accelerate wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import gdown\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (AutoTokenizer, \n",
    "                          AutoModelForSequenceClassification, \n",
    "                          Trainer, \n",
    "                          TrainingArguments, \n",
    "                          EarlyStoppingCallback\n",
    "                          )\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import wandb  # Add wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'train.csv' already exists.\n",
      "File 'val_tweet.csv' already exists.\n",
      "File 'val_label.csv' already exists.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def download_drive_file_by_id(file_id, target_file_name):\n",
    "  dataset_url = f\"https://drive.google.com/u/1/uc?id={file_id}&export=download\"\n",
    "  gdown.download(dataset_url, target_file_name)\n",
    "  print(\"File Download succesfull\")\n",
    "\n",
    "FILE_ID = ['166k7N9KV6jEDvvAr9iLTwrWyfdEcAs5p', '1-2TjS6xPfjWj9YaJGSf-JXXXfNz-2pNT', '1-1k1yHOGP7Wij1mUG2iKaSTN8i1WUgPz']\n",
    "FILENAME = [\"train.csv\", \"val_tweet.csv\", \"val_label.csv\"]\n",
    "\n",
    "\n",
    "for file_id, file_name in zip(FILE_ID, FILENAME):\n",
    "    if not os.path.exists(file_name):\n",
    "        download_drive_file_by_id(file_id, file_name)\n",
    "    else:\n",
    "        print(f\"File '{file_name}' already exists.\")\n",
    "        \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "COL_NAMES = ['index', 'text', 'label']\n",
    "train_df = pd.read_csv(FILENAME[0], header=0, names=COL_NAMES)\n",
    "valid_df_tweet = pd.read_csv(FILENAME[1])\n",
    "valid_df_label = pd.read_csv(FILENAME[2])\n",
    "valid_df = pd.merge(valid_df_tweet, valid_df_label, on='index')\n",
    "valid_df.columns = COL_NAMES\n",
    "train_df.drop('index', axis=1, inplace=True)\n",
    "valid_df.drop('index', axis=1, inplace=True)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "COL_NAMES = ['index', 'text', 'label']\n",
    "train_df = pd.read_csv(FILENAME[0], header=0, names=COL_NAMES)\n",
    "valid_df_tweet = pd.read_csv(FILENAME[1])\n",
    "valid_df_label = pd.read_csv(FILENAME[2])\n",
    "valid_df = pd.merge(valid_df_tweet, valid_df_label, on='index')\n",
    "valid_df.columns = COL_NAMES\n",
    "train_df.drop('index', axis=1, inplace=True)\n",
    "valid_df.drop('index', axis=1, inplace=True)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove hyperlinks\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove unnecessary things\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
    "valid_df['text'] = valid_df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, tokenizer, max_length):\n",
    "    \"\"\"Tokenize the dataset with dynamic max length.\"\"\"\n",
    "    return dataset.map(lambda x: tokenizer(x['text'], padding=\"max_length\", truncation=True, max_length=169), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"xlm-roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    }
   ],
   "source": [
    "api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "wandb.init(project=\"NLP CHIPSAL\", name=get_run_name(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "  def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      gamma (float): Focusing parameter that adjusts the rate at which easy examples are down-weighted.\n",
    "      alpha (list, optional): Weights for each class. Can be used to tackle class imbalance._\n",
    "      reduction (str, optional): Specifies the reduction to apply to the output. Can be 'mean', 'sum', or 'none'.\n",
    "    \"\"\"\n",
    "    super(FocalLoss, self).__init__()\n",
    "    self.gamma = gamma\n",
    "    self.alpha = alpha\n",
    "    self.reduction = reduction\n",
    "\n",
    "  def get_aggregated_loss(self, loss):\n",
    "    if self.reduction == 'mean':\n",
    "      return torch.mean(loss)\n",
    "    elif self.reduction == 'sum':\n",
    "      return torch.sum(loss)\n",
    "    elif self.reduction == 'none':\n",
    "      return loss\n",
    "\n",
    "  def forward(self, inputs, targets):\n",
    "    device = targets.device\n",
    "\n",
    "    # Calculate cross-entropy loss\n",
    "    ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=torch.tensor(self.alpha, device=device))\n",
    "\n",
    "    # Get probabilites from Crossentropy Loss\n",
    "    probs = torch.exp(-ce_loss)\n",
    "\n",
    "    # Compute Focal Loss\n",
    "    focal_loss = ((1 - probs) ** self.gamma) * ce_loss\n",
    "\n",
    "    return self.get_aggregated_loss(focal_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "  def __init__(self, *args, focal_loss_gamma=2.0, focal_loss_alpha=None, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.focal_loss = FocalLoss(gamma=focal_loss_gamma, alpha=focal_loss_alpha)\n",
    "\n",
    "  def compute_loss(self, model, inputs, return_outputs=False):\n",
    "    labels = inputs.pop(\"labels\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    loss = self.focal_loss(logits, labels)\n",
    "    return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"false\" \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def get_run_name(model_name):\n",
    "    return f\"model-{model_name.replace('/', '_')}-{datetime.now().strftime('%Y-%m-%d')}\"\n",
    "\n",
    "run_name = get_run_name(model_name)\n",
    "wandb.run.name = run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"f./{run_name}_results\",\n",
    "    num_train_epochs=5,  # Adjusted number of epochs\n",
    "    per_device_train_batch_size=8,  # Adjusted batch size\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=1e-5,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_total_limit=1,\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=1,  # Adjusted based on batch size\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"wandb\",  # Enable reporting to WandB\n",
    "    run_name=run_name,  # Set the run name dynamically\n",
    "    logging_first_step=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    focal_loss_gamma=4.0,\n",
    "    focal_loss_alpha=torch.tensor([2.0615, 2.5864, 7.7958], dtype=torch.float)\n",
    ")\n",
    "\n",
    "   for name, param in model.named_parameters():\n",
    "        if not param.data.is_contiguous():\n",
    "            param.data = param.data.contiguous()\n",
    "\n",
    "    trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "documentai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
