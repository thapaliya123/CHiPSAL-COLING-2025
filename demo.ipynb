{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the json Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sampled records: 288175\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Path to your large JSON file\n",
    "file_path = 'lit-h.json'\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Total records in the file\n",
    "total_records = 17000000\n",
    "\n",
    "# Desired sample size\n",
    "sample_size = 500000\n",
    "\n",
    "# Regular expressions for Hindi text, URLs, hashtags, mentions, and emojis\n",
    "hindi_pattern = re.compile(r'[\\u0900-\\u097F]')  # Hindi (Devanagari script) characters\n",
    "url_pattern = re.compile(r'http\\S+')\n",
    "hashtag_pattern = re.compile(r'#\\w+')\n",
    "mention_pattern = re.compile(r'@\\w+')\n",
    "emoji_pattern = re.compile(\n",
    "    \"[\\U0001F600-\\U0001F64F]\"  # Emoticons\n",
    "    \"|[\\U0001F300-\\U0001F5FF]\"  # Miscellaneous symbols and pictographs\n",
    "    \"|[\\U0001F680-\\U0001F6FF]\"  # Transport and map symbols\n",
    "    \"|[\\U0001F1E0-\\U0001F1FF]\"  # Flags (iOS)\n",
    "    , flags=re.UNICODE)\n",
    "\n",
    "# Step 1: Generate 250k unique random indices\n",
    "random_indices = sorted(random.sample(range(total_records), sample_size))\n",
    "\n",
    "# Step 2: Function to filter based on Hindi/English content\n",
    "def contains_hindi(text):\n",
    "    # Check if there is Hindi content (Devanagari script)\n",
    "    has_hindi = bool(hindi_pattern.search(text))\n",
    "    \n",
    "    # Clean the text by removing URLs, hashtags, mentions, and emojis\n",
    "    cleaned_text = re.sub(url_pattern, '', text)\n",
    "    cleaned_text = re.sub(hashtag_pattern, '', cleaned_text)\n",
    "    cleaned_text = re.sub(mention_pattern, '', cleaned_text)\n",
    "    cleaned_text = re.sub(emoji_pattern, '', cleaned_text)\n",
    "    \n",
    "    # Check if the remaining text is purely English (only ASCII characters)\n",
    "    is_only_english = all(ord(c) < 128 for c in cleaned_text.strip())\n",
    "    \n",
    "    # Accept the tweet if it has Hindi content or if cleaned text isn't just English\n",
    "    return has_hindi and not is_only_english\n",
    "\n",
    "# Step 3: Open the file and extract records at the random indices\n",
    "sampled_records = []\n",
    "current_index = 0  # Index while reading the file\n",
    "random_index_pointer = 0  # Pointer to the current random index\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        # If current index matches the random index, extract the record\n",
    "        if current_index == random_indices[random_index_pointer]:\n",
    "            record = json.loads(line)\n",
    "            tweet = record.get('tweet', '')\n",
    "\n",
    "            # Filter based on Hindi and non-English content\n",
    "            if contains_hindi(tweet):\n",
    "                sampled_records.append(tweet)\n",
    "            \n",
    "            # Move to the next random index\n",
    "            random_index_pointer += 1\n",
    "\n",
    "            # Break if we have enough sampled records\n",
    "            if random_index_pointer >= len(random_indices):\n",
    "                break\n",
    "\n",
    "        # Increment the current line index\n",
    "        current_index += 1\n",
    "\n",
    "print(f\"Total sampled records: {len(sampled_records)}\")\n",
    "\n",
    "# Step 4: Save the sampled records to a new file with proper encoding\n",
    "with open('tweet_hindi_228k.josn', 'w', encoding='utf-8') as outfile:\n",
    "    for record in sampled_records:\n",
    "        json.dump(record, outfile, ensure_ascii=False)\n",
    "        outfile.write('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Sampled Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loaded records: 288175\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to the saved sample file\n",
    "sample_file_path = 'tweet_hindi_228k.json'\n",
    "\n",
    "# List to hold the loaded records\n",
    "loaded_sampled_records = []\n",
    "\n",
    "# Read the file line by line and load each record\n",
    "with open(sample_file_path, 'r', encoding='utf-8') as infile:\n",
    "    for line in infile:\n",
    "        record = json.loads(line)\n",
    "        loaded_sampled_records.append(record)\n",
    "\n",
    "# Now `loaded_sampled_records` contains all the sampled records\n",
    "print(f\"Total loaded records: {len(loaded_sampled_records)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aisa hai to glt baat ho rhi hai ‡§∏‡§æ‡§∞‡•Ä ‡§à‡§µ‡•Ä‡§è‡§Æ ‡§Æ‡§∂‡•Ä‡§®‡•á ‡§è‡§Ç‡§°‡•ç‡§∞‡•â‡§á‡§° ‡§´‡•ã‡§® ‡§∏‡•á ‡§ï‡§®‡•á‡§ï‡•ç‡§ü ‡§π‡•ã ‡§∞‡§π‡•Ä ‡§π‡•à, ‡§á‡§∏‡§ï‡§æ ‡§Æ‡§§‡§≤‡§¨ ‡§¨‡•Ä‡§ú‡•á‡§™‡•Ä ‡§ú‡•Ä‡§§ ‡§ö‡•Å‡§ï‡•Ä ‡§π‡•à !  http://dhunt.in/57FS0?s=a&ss=twt\\xa0‚Ä¶ via Dailyhunt',\n",
       " 'Sir,  ‡§Ü‡§ú‡§ï‡§≤ ‡§ú‡§¨ ‡§≠‡•Ä ‡§Æ‡•á‡§Ç ‡§Ø‡•á ‡§¶‡•á‡§ñ‡§§‡§æ ‡§π‡•Ç‡§Ç ‡§®‡§æ ‡§ú‡§æ‡§®‡•á ‡§ï‡§ø‡§â ‡§Ü‡§™ ‡§Ø‡§æ‡§¶ ‡§Ü‡§§‡•á ‡§π‡•à, ‡§¨‡§æ‡§§‡§æ ‡§∏‡§ï‡§§‡•á ‡§π‡•à ‡§ï‡§ø‡§â??ü§îü§î pic.twitter.com/QBRZANmr4g',\n",
       " '‡§ú‡§æ‡§ó‡•ã‡§Ç ‡§∞‡•á ‡§≠‡§æ‡§∞‡§§ ‡§ú‡§æ‡§ó‡•ã‡§Ç:  *‡§¶‡•á‡§∂‡§µ‡§æ‡§∏‡§ø‡§Ø‡•ã‡§Ç ‡§≠‡•Ä‡§°‡§º ‡§ú‡•Å‡§ü‡§æ‡§ï‡§∞ ‡§°‡§∞‡§æ‡§®‡•á ‡§ï‡§æ ‡§¨‡§π‡§æ‡§®‡§æ ‡§¢‡•Ç‡§Ç‡§¢ ‡§≤‡§ø‡§Ø‡§æ ‡§Æ‡•Å‡§¶‡•ç‡§¶‡•ã‡§Ç ‡§∏‡•á ‡§ß‡•ç‡§Ø‡§æ‡§® ‡§π‡§ü‡§æ‡§®‡•á ‡§ï‡§æ ‡§¨‡§π‡§æ‡§®‡§æ ‡§¢‡•Ç‡§Ç‡§¢ ‡§≤‡§ø‡§Ø‡§æ ‡§ï‡§≤ ‡§§‡§ï ‡§ú‡•ã ‡§§‡§∞‡§ï‡•ç‡§ï‡•Ä ‡§ï‡•Ä ‡§¨‡§æ‡§§ ‡§ï‡§∞‡§§‡•á ‡§•‡•á ‡§ö‡•Å‡§®‡§æ‡§µ ‡§Ü‡§§‡•á ‡§π‡•Ä ‡§Æ‡§®‡•ç‡§¶‡§ø‡§∞ ‡§ï‡§æ ‡§¨‡§π‡§æ‡§®‡§æ ‡§¢‡•Ç‡§Ç‡§¢ ‡§≤‡§ø‡§Ø‡§æ ‡§π‡•à!  *‡§ú‡§æ‡§ó‡•ã‡§Ç ‡§Æ‡•á‡§∞‡•á ‡§¶‡•á‡§∂‡§µ‡§æ‡§∏‡§ø‡§Ø‡•ã‡§Ç ‡§ú‡§æ‡§ó‡•ã‡§Ç ‡§µ‡§ï‡•ç‡§§ ‡§π‡•à ‡§¨‡§¶‡§≤‡§æ‡§µ ‡§ï‡§æ 2019 ‡§Æ‡•á ‡§≤‡§π‡§∞ ‡§ñ‡§º‡§§‡•ç‡§Æ ‡§Ö‡§¨‡§ï‡•Ä‡§¨‡§æ‡§∞ RG ‡§ú‡•Ä ‡§ï‡•Ä ‡§Ü‡§Å‡§ß‡•Ä ‡§π‡•à !',\n",
       " '@myogiadityanath ‡§∏‡•ç‡§µ‡§æ‡§Æ‡•Ä ‡§ú‡•Ä 68500 2019  ‡§≠‡§∞‡•ç‡§§‡•Ä ‡§Æ‡•á‡§Ç ‡§ï‡§ü‡§ë‡§´ ‡§¨‡§§‡§æ‡§á‡§Ø‡•á ‡§™‡•ç‡§≤‡•Ä‡§ú‡§º üôèüôèüôèüôèüôèüôèüôèüôè pic.twitter.com/tAL0AEySC1',\n",
       " '‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§Æ‡•á ‡§Ö‡§¨ 68% ‡§Ü‡§∞‡§ï‡•ç‡§∑‡§£ ‡§π‡•ã‡§®‡•á ‡§µ‡§æ‡§≤‡§æ  ‡§Ö‡§¨ ‡§™‡§∞‡§ø‡§µ‡§æ‡§∞ ‡§®‡§ø‡§Ø‡•ã‡§ú‡§® ‡§ï‡•ç‡§Ø‡•ã‡§Ç ‡§Ö‡§™‡§®‡§æ‡§è‡§Ç? ‡§Ö‡§¨ ‡§™‡§¢‡§º‡§®‡•á ‡§≤‡§ø‡§ñ‡§®‡•á ‡§ï‡•Ä ‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï‡§§‡§æ ‡§π‡•à?  ‡§ú‡§®‡§∏‡§Å‡§ñ‡•ç‡§Ø‡§æ ‡§¨‡§¢‡§º‡§æ‡§ì ‡§™‡•ç‡§∞‡§¶‡§∞‡•ç‡§∂‡§® ‡§ï‡§∞ ‡§§‡§æ‡§ï‡§§ ‡§¶‡§ø‡§ñ‡§æ‡§ì  ‡§∏‡§Ç‡§µ‡§ø‡§ß‡§æ‡§® ‡§∏‡§Ç‡§∏‡•ã‡§ß‡§® ‡§ï‡§∞ ‡§Ø‡•ã‡§ú‡§®‡§æ‡§ì‡§Ç ‡§Æ‡•á‡§Ç ‡§Ü‡§∞‡§ï‡•ç‡§∑‡§£ ‡§≤‡•ã  ‡§ú‡•ç‡§û‡§æ‡§®‡•Ä ‡§®‡§π‡•Ä‡§Å, ‡§¶‡§≤‡§ø‡§§ ‡§¶‡§ø‡§ñ‡•ã ‡§§‡§æ‡§ï‡§§‡§µ‡§∞ ‡§®‡§π‡•Ä, ‡§ï‡§Æ‡§ú‡•ã‡§∞ ‡§¶‡§ø‡§ñ‡•ã  ‡§¨‡§°‡§º‡§æ ‡§¨‡•à‡§Ç‡§ï, ‡§µ‡•ã‡§ü ‡§¨‡•à‡§Ç‡§ï ‡§ï‡•ã‡§à ‡§ü‡•à‡§ï‡•ç‡§∏ ‡§¶‡•á,‡§Æ‡§ú‡•á ‡§ï‡•ã‡§à ‡§ï‡§∞‡•á‡§Ç',\n",
       " '#SundayMotivation ‡§ó‡•Å‡§∞‡•Å ‡§ï‡•á ‡§¨‡§ø‡§®‡§æ ‡§Æ‡•ã‡§ï‡•ç‡§∑ ‡§∏‡§Æ‡•ç‡§≠‡§µ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§ ‡§ó‡•Å‡§∞‡•Å ‡§≠‡•Ä ‡§™‡•Ç‡§∞‡§æ ‡§π‡•ã‡•§‡§®‡§ï‡§≤‡•Ä ‡§ó‡•Å‡§∞‡•Å‡§ì‡§Ç ‡§∏‡•á ‡§≠‡•Ä ‡§Æ‡•ã‡§ï‡•ç‡§∑ ‡§∏‡§Æ‡•ç‡§≠‡§µ ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§  ‡§Ö‡§µ‡§∂‡•ç‡§Ø ‡§¶‡•á‡§ñ‡•á‡§Ç :-‡§à‡§∂‡•ç‡§µ‡§∞ t v ‡§∞‡§æ‡§§ 8:30 ‡§¨‡§ú‡•á ‡§∏‡•á‡•§',\n",
       " '‡§¨‡•á‡§≤‡§® ‡§≠‡•Ä ‡§è‡§ï ‡§¨‡§°‡§º‡§æ ‡§π‡•Ä ‡§µ‡§ø‡§ö‡§ø‡§§‡•ç‡§∞ ‡§Ø‡§Ç‡§§‡•ç‡§∞ ‡§π‡•à ‡§ú‡§ø‡§∏‡§∏‡•á ‡§∞‡•ã‡§ü‡•Ä ‡§ó‡•ã‡§≤ ‡§π‡•ã‡§§‡•Ä ‡§π‡•à ‡§î‡§∞ ‡§™‡§§‡§ø ‡§∏‡•Ä‡§ß‡§æ !!',\n",
       " '‡§¨‡•á‡§ö‡§æ‡§∞‡•á ‡§ï‡§ø‡§§‡§®‡§æ ‡§ó‡§∞‡•Ä‡§¨ ‡§ï‡§ø‡§∏‡§æ‡§® ‡§π‡•à',\n",
       " '‡§ú‡•Å‡§ó ‡§ú‡•Å‡§ó ‡§ú‡§ø‡§ì ‡§π‡§ú‡§æ‡§∞‡•ã‡§Ç ‡§∏‡§æ‡§≤ ‡§Ø‡•á ‡§¶‡§ø‡§® ‡§Ü‡§Ø‡•á ‡§¨‡§æ‡§∞‡§Æ‡•ç‡§¨‡§æ‡§∞  ‡§∂‡§æ‡§¶‡•Ä ‡§ï‡•Ä ‡§∏‡§æ‡§≤‡§ó‡§ø‡§∞‡§π ‡§Æ‡•Å‡§¨‡§æ‡§∞‡§ï ‡§π‡•ã',\n",
       " '‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø ‡§Ü‡§™‡§ï‡•á ‡§®‡•á‡§§‡§æ ‡§ú‡•à‡§∏‡•á ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à  ‡§ö‡§æ‡§Ø ‡§µ‡§æ‡§≤‡•á ‡§î‡§∞ ‡§™‡§¢‡•á ‡§≤‡§ø‡§ñ‡•á ‡§ï‡§æ ‡§´‡§∞‡•ç‡§ï ‡§Ø‡§π‡•Ä ‡§π‡•ã‡§§‡§æ ‡§π‡•à']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_sampled_records[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
