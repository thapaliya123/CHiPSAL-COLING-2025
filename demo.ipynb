{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the json Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "\n",
    "# Path to your large JSON file\n",
    "file_path = 'lit-h.json'\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Total records in the file\n",
    "total_records = 17000000\n",
    "\n",
    "# Desired sample size\n",
    "sample_size = 500000\n",
    "\n",
    "# Regular expressions for Hindi text, URLs, hashtags, mentions, and emojis\n",
    "hindi_pattern = re.compile(r'[\\u0900-\\u097F]')  # Hindi (Devanagari script) characters\n",
    "url_pattern = re.compile(r'http\\S+')\n",
    "hashtag_pattern = re.compile(r'#\\w+')\n",
    "mention_pattern = re.compile(r'@\\w+')\n",
    "emoji_pattern = re.compile(\n",
    "    \"[\\U0001F600-\\U0001F64F]\"  # Emoticons\n",
    "    \"|[\\U0001F300-\\U0001F5FF]\"  # Miscellaneous symbols and pictographs\n",
    "    \"|[\\U0001F680-\\U0001F6FF]\"  # Transport and map symbols\n",
    "    \"|[\\U0001F1E0-\\U0001F1FF]\"  # Flags (iOS)\n",
    "    , flags=re.UNICODE)\n",
    "\n",
    "# Step 1: Generate 250k unique random indices\n",
    "random_indices = sorted(random.sample(range(total_records), sample_size))\n",
    "\n",
    "# Step 2: Function to filter based on Hindi/English content\n",
    "def contains_hindi(text):\n",
    "    # Check if there is Hindi content (Devanagari script)\n",
    "    has_hindi = bool(hindi_pattern.search(text))\n",
    "    \n",
    "    # Clean the text by removing URLs, hashtags, mentions, and emojis\n",
    "    cleaned_text = re.sub(url_pattern, '', text)\n",
    "    cleaned_text = re.sub(hashtag_pattern, '', cleaned_text)\n",
    "    cleaned_text = re.sub(mention_pattern, '', cleaned_text)\n",
    "    cleaned_text = re.sub(emoji_pattern, '', cleaned_text)\n",
    "    \n",
    "    # Check if the remaining text is purely English (only ASCII characters)\n",
    "    is_only_english = all(ord(c) < 128 for c in cleaned_text.strip())\n",
    "    \n",
    "    # Accept the tweet if it has Hindi content or if cleaned text isn't just English\n",
    "    return has_hindi and not is_only_english\n",
    "\n",
    "# Step 3: Open the file and extract records at the random indices\n",
    "sampled_records = []\n",
    "current_index = 0  # Index while reading the file\n",
    "random_index_pointer = 0  # Pointer to the current random index\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        # If current index matches the random index, extract the record\n",
    "        if current_index == random_indices[random_index_pointer]:\n",
    "            record = json.loads(line)\n",
    "            tweet = record.get('tweet', '')\n",
    "\n",
    "            # Filter based on Hindi and non-English content\n",
    "            if contains_hindi(tweet):\n",
    "                sampled_records.append(tweet)\n",
    "            \n",
    "            # Move to the next random index\n",
    "            random_index_pointer += 1\n",
    "\n",
    "            # Break if we have enough sampled records\n",
    "            if random_index_pointer >= len(random_indices):\n",
    "                break\n",
    "\n",
    "        # Increment the current line index\n",
    "        current_index += 1\n",
    "\n",
    "print(f\"Total sampled records: {len(sampled_records)}\")\n",
    "\n",
    "# Step 4: Save the sampled records to a new file with proper encoding\n",
    "with open('tweet_hindi_228k.josn', 'w', encoding='utf-8') as outfile:\n",
    "    for record in sampled_records:\n",
    "        json.dump(record, outfile, ensure_ascii=False)\n",
    "        outfile.write('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Sampled Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to the saved sample file\n",
    "sample_file_path = 'tweet_hindi_228k.json'\n",
    "\n",
    "# List to hold the loaded records\n",
    "loaded_sampled_records = []\n",
    "\n",
    "# Read the file line by line and load each record\n",
    "with open(sample_file_path, 'r', encoding='utf-8') as infile:\n",
    "    for line in infile:\n",
    "        record = json.loads(line)\n",
    "        loaded_sampled_records.append(record)\n",
    "\n",
    "# Now `loaded_sampled_records` contains all the sampled records\n",
    "print(f\"Total loaded records: {len(loaded_sampled_records)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_sampled_records[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nepberta Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 1t8TL86ozNi4YabpnfThjxRhbaYBg8F0P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip clean_nepberta_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('clean_date_categories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics = df[df['clean_categories'] == 'politics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(n=1000000, random_state=42).to_csv('50lakh samples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.clean_categories.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['clean_categories'] == 'society'].sample(n=100).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in ./venv/lib/python3.12/site-packages (0.25.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.12/site-packages (from huggingface_hub) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.12/site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.12/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.12/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./venv/lib/python3.12/site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.12/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests->huggingface_hub) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests->huggingface_hub) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export HF_TOKEN=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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in ./venv/lib/python3.12/site-packages (0.25.1)\n",
      "Requirement already satisfied: datasets in ./venv/lib/python3.12/site-packages (3.0.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.12/site-packages (from huggingface_hub) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.12/site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.12/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.12/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./venv/lib/python3.12/site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.12/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venv/lib/python3.12/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in ./venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in ./venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in ./venv/lib/python3.12/site-packages (from datasets) (3.10.9)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests->huggingface_hub) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests->huggingface_hub) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load from a local CSV file\n",
    "dataset = load_dataset(\"csv\", data_files=\"/home/suman/CHIPSAL-COLING-2025/50lakh samples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df5604e2ebe40028f65054effcc63d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1461bad8d331430cb1a49d0e5f9d1a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/91 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82fc51d30a3d477aa432b6abff76d990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/91 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c4b8cd1aa44cb59c89e1698abfa845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/91 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd8fb7d83bd47f5afa4c5a05f1ba384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/91 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b26111783e45e691d88c01f7a05f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/91 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82742596ac44dd78ebffe9e2fd67b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/91 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45113a5ee11d46b7ba3b3865bbf53c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/91 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16956d8aa254f0a9b788b6cc405a9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/91 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6622dc25fefe44f98665985fa16789d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/91 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8b7a617fa84ee7a960a9678d801f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/91 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd623623e34143e59c86ffa501d60914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/91 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7811c1a11548c1930a672204e2550b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/31.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/sumanpaudel1997/tweet_data_5gb/commit/42b101b0c4819ee7b7151f8a75ec97d08e9967e3', commit_message='Upload dataset', commit_description='', oid='42b101b0c4819ee7b7151f8a75ec97d08e9967e3', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/sumanpaudel1997/tweet_data_5gb', endpoint='https://huggingface.co', repo_type='dataset', repo_id='sumanpaudel1997/tweet_data_5gb'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.push_to_hub(\"sumanpaudel1997/tweet_data_5gb\", private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('tweet_hindi_288k.json', lines=True)\n",
    "df.columns = ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aisa hai to glt baat ho rhi hai सारी ईवीएम मशी...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sir,  आजकल जब भी में ये देखता हूं ना जाने किउ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>जागों रे भारत जागों:  *देशवासियों भीड़ जुटाकर ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@myogiadityanath स्वामी जी 68500 2019  भर्ती म...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>महाराष्ट्र मे अब 68% आरक्षण होने वाला  अब परिव...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288170</th>\n",
       "      <td>Retweeted चाचा अभय (@Roflchacha):  पेट्रोंल पं...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288171</th>\n",
       "      <td>इसीलिए कहा कि न्यूज चैनल देखना बंद करें और आमज...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288172</th>\n",
       "      <td>बिधूना बिजली विभाग के सरकारी कार्यलय मैं लाखों...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288173</th>\n",
       "      <td>पूर्व क्रिकेटर जयसूर्या ने लीक किया Ex-Wife के...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288174</th>\n",
       "      <td>जूही में बिरयानी वाले फर्जी ने बच्चों को बताया...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288175 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text\n",
       "0       Aisa hai to glt baat ho rhi hai सारी ईवीएम मशी...\n",
       "1       Sir,  आजकल जब भी में ये देखता हूं ना जाने किउ ...\n",
       "2       जागों रे भारत जागों:  *देशवासियों भीड़ जुटाकर ...\n",
       "3       @myogiadityanath स्वामी जी 68500 2019  भर्ती म...\n",
       "4       महाराष्ट्र मे अब 68% आरक्षण होने वाला  अब परिव...\n",
       "...                                                   ...\n",
       "288170  Retweeted चाचा अभय (@Roflchacha):  पेट्रोंल पं...\n",
       "288171  इसीलिए कहा कि न्यूज चैनल देखना बंद करें और आमज...\n",
       "288172  बिधूना बिजली विभाग के सरकारी कार्यलय मैं लाखों...\n",
       "288173  पूर्व क्रिकेटर जयसूर्या ने लीक किया Ex-Wife के...\n",
       "288174  जूही में बिरयानी वाले फर्जी ने बच्चों को बताया...\n",
       "\n",
       "[288175 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "\n",
    "def encode_emoji(text):\n",
    "    return emoji.demojize(text)\n",
    "df['text'] = df['text'].apply(encode_emoji)\n",
    "df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('tweet_hindi_288k.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9591dbedf6f48f6be3cea96be129514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0e0742a49e434a8c525ba6599d3fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5da4d82f0724882853d7f1b8fdca68a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/289 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/sumanpaudel1997/hindi_tweet_sampled_dataset/commit/353d7809c62dbcb95b2c1c915b05a993b4d23194', commit_message='Upload dataset', commit_description='', oid='353d7809c62dbcb95b2c1c915b05a993b4d23194', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/sumanpaudel1997/hindi_tweet_sampled_dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='sumanpaudel1997/hindi_tweet_sampled_dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=\"/home/suman/CHIPSAL-COLING-2025/tweet_hindi_288k.csv\")\n",
    "dataset.push_to_hub(\"sumanpaudel1997/hindi_tweet_sampled_dataset\", private=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
